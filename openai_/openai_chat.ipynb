{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat Completions API\n",
    "https://platform.openai.com/docs/overview  \n",
    "https://platform.openai.com/docs/api-reference/chat  \n",
    "\n",
    "배포된 openai의 api key를 .env의 OPENAI_API_KEY에 등록하여 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REST API 요청\n",
    "라이브러리 없이 직접 HTTP 통신을 통해 api를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 친절한 AI 강사입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Chat Completions API가 뭐야? 2~3문장으로 답변해줘\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(URL, headers=headers, json=payload)\n",
    "pprint(response.json())\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI SDK를 활용한 요청\n",
    "공식 라이브러리를 사용하여 생산성을 높이는 표준 방식이다.  \n",
    "`pip install openai` 를 통해 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Openai SDK를 사용하면 어떤 점이 좋아?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt 비교\n",
    "\n",
    "동일한 질문에 대해 AI의 페르소나에 따라 답변이 어떻게 달라지는지 확인해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"아침 일찍 일어나는 습관의 장점에 대해 말해줘.\"\n",
    "\n",
    "personas = {\n",
    "    \"열정적인 셰프\": \"당신은 요리에 인생을 건 셰프입니다. 인생의 모든 이치를 요리 과정과 재료에 비유하여 설명하세요.\",\n",
    "    \"엄격한 헬스 트레이너\": \"당신은 매우 엄격한 운동 전문가입니다. 강한 어조로 자기관리를 강조하며 답변하세요.\",\n",
    "    \"지혜로운 판다\": \"당신은 대나무 숲에 사는 느긋하고 지혜로운 판다입니다. 느릿느릿하고 평화로운 말투로 조언을 건네세요.\"\n",
    "}\n",
    "\n",
    "for name, prompt in personas.items():\n",
    "    print(f\"--- [{name}] 버전 ---\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}, # prompt 는 personas 딕셔너리의 벨류값\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature 비교\n",
    "\n",
    "동일한 질문에 대해 temperature에 따라 답변이 어떻게 달라지는지 확인해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_topic = \"운동화 브랜드의 새로운 슬로건을 5개 제안해줘. 단, '속도'나 '승리' 같은 뻔한 단어는 제외하고 아주 기발하게 작성해줘.\"\n",
    "temperatures = [0.3, 0.8, 1.0, 1.3, 1.5, 1.6, 1.8]\n",
    "\n",
    "for t in temperatures:\n",
    "    print(f\"### 설정값 (Temperature): {t} ###\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": creative_topic}],\n",
    "        temperature=t,\n",
    "        max_completion_tokens=200, \n",
    "        timeout=15.0\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_topic = \"우리집 강아지의 별명을 3개 지어줘.\"\n",
    "temperatures = [0.3, 0.8, 1.0, 1.3, 1.5, 1.6, 1.8]\n",
    "\n",
    "for t in temperatures:\n",
    "    print(f\"### 설정값 (Temperature): {t} ###\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": creative_topic}],\n",
    "        temperature=t,\n",
    "        max_completion_tokens=200, \n",
    "        timeout=15.0\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `messages` 배열을 활용한 대화 맥락 유지 (Context Window)\n",
    "Chat Completions API는 상태를 저장하지 않는(Stateless) 방식이므로, 이전 대화 내역을 리스트에 계속 누적해서 보내야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_without_memory(user_input):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 3. 모델의 답변을 기록에 추가 (이것이 맥락 유지의 핵심)\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# 실습 테스트\n",
    "print(\"Q1: 내 이름은 jun이야.\")\n",
    "print(f\"A1: {chat_without_memory('내 이름은 jun이야')}\\n\")\n",
    "\n",
    "print(\"Q2: 내 이름이 뭐라고?\")\n",
    "print(f\"A2: {chat_without_memory('내 이름이 뭐라고?')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 내 이름은 jun이야.\n",
      "A1: 반가워, Jun! 어떻게 도와드릴까요?\n",
      "\n",
      "Q2: 내 이름이 뭐라고?\n",
      "A2: Jun이야! 맞지?\n"
     ]
    }
   ],
   "source": [
    "# 대화 내역을 저장할 리스트 초기화\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 사용자의 이름을 기억하는 비서입니다.\"}\n",
    "]\n",
    "\n",
    "def chat_with_memory(user_input):\n",
    "    # 1. 사용자 질문을 기록에 추가\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # 2. 전체 기록을 API에 전송\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=history\n",
    "    )\n",
    "    \n",
    "    # 3. 모델의 답변을 기록에 추가 (이것이 맥락 유지의 핵심)\n",
    "    answer = response.choices[0].message.content\n",
    "    history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# 실습 테스트\n",
    "print(\"Q1: 내 이름은 jun이야.\")\n",
    "print(f\"A1: {chat_with_memory('내 이름은 jun이야.')}\\n\")\n",
    "\n",
    "print(\"Q2: 내 이름이 뭐라고?\")\n",
    "print(f\"A2: {chat_with_memory('내 이름이 뭐라고?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs (구조화된 출력)\n",
    "모델의 답변을 단순히 텍스트로 받는 것이 아니라, JSON 형태로 고정하여 받을 수 있다.  \n",
    "웹 서비스의 백엔드에서 데이터를 바로 처리해야 할 때 필수적인 기능이다.  \n",
    "여기서는 `JSON mode(json_object)`로 json format을 활용하지만,  \n",
    "이후에는 pydantic 라이브러리를 활용한 `JSON Scheme` 방식을 통해 명확한 json 응답 형식을 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"너는 요리사야. 답변은 반드시 JSON 형식으로 해줘.\"},\n",
    "        {\"role\": \"user\", \"content\": \"떡볶이 레시피 알려줘.\"}\n",
    "    ],\n",
    "    # JSON 모드 활성화\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "# 문자열로 온 답변을 직접 파싱해야 함\n",
    "res_json = json.loads(response.choices[0].message.content)\n",
    "print(res_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming (실시간 응답 처리)\n",
    "stream=True 설정을 통해 활성화한다.  \n",
    "서버는 SSE(Server-Sent Events) 프로토콜을 사용하여 응답을 끊지 않고 조각(Chunk) 단위로 지속적으로 전송한다.  \n",
    "응답 객체는 제너레이터 형식으로, for 루프를 사용해 활용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"양자 역학에 대해 초등학생도 이해할 수 있게 설명해줘.\"\n",
    "print(f\"질문: {prompt}\\n\")\n",
    "print(\"답변: \", end=\"\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=True \n",
    ")\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True) # flush 옵션을 통해 출력 버퍼를 즉시 비워 스트리밍 답변이 지연 없이 실시간으로 표시되도록 한다.\n",
    "        full_response += content\n",
    "\n",
    "print(\"\\n\\n--- 스트리밍 종료 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비동기 요청\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "async_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "async def get_food_recommendation(city):\n",
    "    print(f\"[{city}] 맛집 검색 시작...\")\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{city}에 가면 꼭 먹어야 할 음식 딱 한 가지만 추천해줘.\"}]\n",
    "    )\n",
    "    print(f\"[{city}] 검색 완료!\")\n",
    "    return f\"{city}: {response.choices[0].message.content}\"\n",
    "\n",
    "async def main():\n",
    "    cities = [\"서울\", \"파리\", \"뉴욕\", \"도쿄\", \"방콕\", \"로마\"]\n",
    "    tasks = [get_food_recommendation(c) for c in cities]\n",
    "    \n",
    "    # 여러 요청을 동시에(병렬로) 처리\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(\"\\n--- [여행자들을 위한 미식 가이드] ---\")\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprobs - 확률 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 새로 오픈한 조용한 북카페 이름을 한글로 딱 하나만 추천해줘.\n",
      "답변: \"여유로운 페이지\"는 어떠세요? 조용한 분위기가 느껴지면서 책을 읽는 여유를 표현한 이름입니다.\n",
      "\n",
      "Token           | Probability  | Top Alternatives\n",
      "------------------------------------------------------------\n",
      "\"               |      98.57% | \"(98.6%), '(1.1%), “(0.2%)\n",
      "여               |      18.28% | 책(23.5%), 여(18.3%), 조(18.3%)\n",
      "유               |      77.50% | 유(77.5%), 백(22.2%), 운(0.2%)\n",
      "로운              |      52.89% | 로운(52.9%), 의(19.5%), \"(10.4%)\n",
      " 페이지            |      61.32% |  페이지(61.3%),  책(32.8%),  서(3.1%)\n",
      "\"               |     100.00% | \"(100.0%), 들(0.0%), ”(0.0%)\n",
      "는               |      25.55% | 는(25.5%), <|end|>(25.5%),  \\xec\\x96\\xb4\\xeb\\x96(22.5%)\n",
      " \\xec\\x96\\xb4\\xeb\\x96 |      95.77% |  \\xec\\x96\\xb4\\xeb\\x96(95.8%),  어떤(3.7%),  어(0.5%)\n",
      "\\xa0            |      93.24% | \\xa0(93.2%), \\xa8(6.8%), \\x84(0.0%)\n",
      "세요              |      49.98% | 신(50.0%), 세요(50.0%), 실(0.0%)\n",
      "?               |     100.00% | ?(100.0%), ?\n",
      "(0.0%), ?\n",
      "\n",
      "(0.0%)\n",
      " 조              |      54.07% |  조(54.1%), <|end|>(32.8%),  북(5.0%)\n",
      "용               |     100.00% | 용(100.0%),  quiet(0.0%), 用(0.0%)\n",
      "한               |      51.94% | 한(51.9%), 하고(45.8%), 함(1.2%)\n",
      " 분위             |      94.54% |  분위(94.5%),  북(5.3%),  느낌(0.0%)\n",
      "기가              |       0.38% | 기(72.3%), 기를(23.5%), 기에(3.2%)\n",
      " 느              |      43.57% |  잘(55.9%),  느(43.6%),  책(0.1%)\n",
      "껴               |     100.00% | 껴(100.0%), 려(0.0%), 겨(0.0%)\n",
      "지               |      48.18% | 지(48.2%), 지는(42.5%), 지고(7.4%)\n",
      "면서              |      95.79% | 면서(95.8%), 며(4.2%), 기도(0.0%)\n",
      " 책              |      19.80% | 도(69.1%),  책(19.8%),  독(9.4%)\n",
      "을               |      40.52% | 과(52.0%), 을(40.5%), 의(4.3%)\n",
      " 읽              |      84.19% |  읽(84.2%),  즐(8.9%),  여(1.2%)\n",
      "는               |      76.60% | 는(76.6%), 기에(15.1%), 기(3.4%)\n",
      " 여              |      51.97% |  여(52.0%),  즐(27.8%),  편(10.2%)\n",
      "유               |     100.00% | 유(100.0%), 정을(0.0%), 백(0.0%)\n",
      "를               |      92.47% | 를(92.5%), 로(5.9%), 로운(1.2%)\n",
      " 표현             |      19.03% |  강조(40.3%),  잘(19.0%),  표현(19.0%)\n",
      "한               |      69.42% | 한(69.4%), 할(17.6%), 하는(8.3%)\n",
      " 이름             |      99.92% |  이름(99.9%),  \\xeb\\xa9(0.0%),  느낌(0.0%)\n",
      "입니다             |      99.62% | 입니다(99.6%), 이에(0.3%), 이(0.0%)\n",
      ".               |     100.00% | .(100.0%), .\n",
      "(0.0%), !(0.0%)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "prompt = \"새로 오픈한 조용한 북카페 이름을 한글로 딱 하나만 추천해줘.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    logprobs=True,\n",
    "    top_logprobs=3,\n",
    "    max_completion_tokens=50\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "logprobs_data = response.choices[0].logprobs.content\n",
    "\n",
    "print(f\"질문: {prompt}\")\n",
    "print(f\"답변: {content}\\n\")\n",
    "print(f\"{'Token':<15} | {'Probability':<12} | {'Top Alternatives'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for lp in logprobs_data:\n",
    "    prob = math.exp(lp.logprob) * 100\n",
    "    alternatives = [f\"{top.token}({math.exp(top.logprob)*100:.1f}%)\" for top in lp.top_logprobs]\n",
    "    print(f\"{lp.token:<15} | {prob:>10.2f}% | {', '.join(alternatives)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
